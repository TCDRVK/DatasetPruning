{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y pred 0    21181\n",
      "1     2350\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y pred 0    21180\n",
      "1     2351\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y pred 0    21184\n",
      "1     2347\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y pred 0    21184\n",
      "1     2347\n",
      "dtype: int64\n",
      "y pred 0    21185\n",
      "1     2347\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiruthika Velusamy\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from pyod.models.knn import KNN \n",
    "from sklearn.model_selection import KFold\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "\n",
    "def str_to_numeric(Data):\n",
    "\n",
    " label_mapping = {}\n",
    " char_cols = Data.dtypes.pipe(lambda x: x[x == 'object']).index\n",
    " for c in char_cols:\n",
    "    Data[c], label_mapping[c] = pd.factorize(Data[c])\n",
    "\n",
    " return Data\n",
    "\n",
    "def load_data():\n",
    "    #loading the data set from CSV file\n",
    "    data = pd.read_csv(\"C:/Users/Kiruthika Velusamy/Documents/Machine Learning Assignment/cleaned_and_imputed_data.csv\");\n",
    "   \n",
    "    #data =data.drop(data.columns[0],axis=1)\n",
    "  \n",
    "    return data\n",
    "\n",
    "def split_train_test(data):\n",
    "   \n",
    "    original_train, original_test = train_test_split(data, train_size=0.8)\n",
    "    train = str_to_numeric(original_train)\n",
    "    test = str_to_numeric(original_test)\n",
    "    #train data\n",
    "    return train,test\n",
    "    \n",
    "def LOF(train_X):\n",
    "    clf=LocalOutlierFactor(n_neighbors=100, metric='minkowski',contamination=0.1)\n",
    "    y_pred = clf.fit_predict(train_X.drop(['id'], axis=1))\n",
    "\n",
    "    outlier=y_pred[y_pred==-1]\n",
    "    inlier=y_pred[y_pred==1]\n",
    "    print('inlier',len(inlier),'outlier',len(outlier),'total train X', len(train_X))\n",
    "\n",
    "def HDBSCAN(train_X,train_Y):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=25, gen_min_span_tree=True)\n",
    "    clusterer.fit(train_X)\n",
    "    print(clusterer.outlier_scores_)\n",
    "    print(pd.Series(clusterer.labels_).value_counts())\n",
    "    print(clusterer.labels_.max())\n",
    "    sns.distplot(clusterer.outlier_scores_[np.isfinite(clusterer.outlier_scores_)], rug=True)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.9)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    color_palette = sns.color_palette('Paired', 12)\n",
    "    cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer.probabilities_)]\n",
    "def KNN_outlier(train_X): \n",
    "#KNN using pypod\n",
    "    clf_name = 'KNN'\n",
    "    clf = KNN()\n",
    "    clf.fit(train_X)\n",
    "    y_train_scores = clf.decision_scores_\n",
    "    y_train_pred = clf.labels_\n",
    "    print('y pred',pd.Series(y_train_pred).value_counts())\n",
    "    \n",
    "# def KFold_outlier(X,Y):\n",
    "#     kf = KFold(len(Y), n_folds=5)\n",
    "#     for train_index, test_index in kf:\n",
    "#        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "#        LOF(X_train,y_train)\n",
    "       \n",
    "    \n",
    "def Outlier_detection():\n",
    "     #loading the imputed data set from CSV file\n",
    "    data=load_data()\n",
    "    train,test=split_train_test(data)\n",
    "     #splitting the dataset into train and test\n",
    "  \n",
    "    train_X = train[train.columns.difference(['y'])]\n",
    "    train_Y = train[['y']]\n",
    "    kf = KFold(n_splits=5)\n",
    "#     for train_index, test_index in kf.split(train_X):\n",
    "#        X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]\n",
    "    KNN_outlier(X_train)\n",
    "\n",
    "        LOF(train_X,train_Y)\n",
    "    #HDB scan outlier detection\n",
    "#     HDBSCAN(train_X,train_Y)\n",
    "    #KNN detection\n",
    "#     KNN_outlier(train_X, train_Y)\n",
    "    \n",
    "    \n",
    "Outlier_detection()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     sns.distplot(clusterer.outlier_scores_[np.isfinite(clusterer.outlier_scores_)], rug=True)\n",
    "#     threshold = pd.Series(clusterer.outlier_scores_).quantile(0.9)\n",
    "#     outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "#     color_palette = sns.color_palette('Paired', 12)\n",
    "#     cluster_colors = [color_palette[x] if x >= 0\n",
    "#                   else (0.5, 0.5, 0.5)\n",
    "#                   for x in clusterer.labels_]\n",
    "#     cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "#                          zip(cluster_colors, clusterer.probabilities_)]\n",
    "#     plt.scatter(*train_X.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
    "#     plt.scatter(*train_X.T, s=50, linewidth=0)\n",
    "#     plt.scatter(*train_X[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draft wihout k fold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from pyod.models.knn import KNN \n",
    "from sklearn.model_selection import KFold\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "\n",
    "def str_to_numeric(Data):\n",
    "\n",
    " label_mapping = {}\n",
    " char_cols = Data.dtypes.pipe(lambda x: x[x == 'object']).index\n",
    " for c in char_cols:\n",
    "    Data[c], label_mapping[c] = pd.factorize(Data[c])\n",
    "\n",
    " return Data\n",
    "\n",
    "def load_data():\n",
    "    #loading the data set from CSV file\n",
    "    data = pd.read_csv(\"C:/Users/Kiruthika Velusamy/Documents/Machine Learning Assignment/cleaned_and_imputed_data.csv\");\n",
    "   \n",
    "    #data =data.drop(data.columns[0],axis=1)\n",
    "  \n",
    "    return data\n",
    "\n",
    "def split_train_test(data):\n",
    "   \n",
    "    original_train, original_test = train_test_split(data, train_size=0.8)\n",
    "    train = str_to_numeric(original_train)\n",
    "    test = str_to_numeric(original_test)\n",
    "    #train data\n",
    "    return train,test\n",
    "    \n",
    "def LOF(train_X):\n",
    "    clf=LocalOutlierFactor(n_neighbors=100, metric='minkowski',contamination=0.1)\n",
    "    y_pred = clf.fit_predict(train_X.drop(['id'], axis=1))\n",
    "\n",
    "    outlier=y_pred[y_pred==-1]\n",
    "    inlier=y_pred[y_pred==1]\n",
    "    print('inlier',len(inlier),'outlier',len(outlier),'total train X', len(train_X))\n",
    "\n",
    "def HDBSCAN(train_X):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=25, gen_min_span_tree=True)\n",
    "    clusterer.fit(train_X)\n",
    "    print(clusterer.outlier_scores_)\n",
    "    print(pd.Series(clusterer.labels_).value_counts())\n",
    "    print(clusterer.labels_.max())\n",
    "    sns.distplot(clusterer.outlier_scores_[np.isfinite(clusterer.outlier_scores_)], rug=True)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.9)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    color_palette = sns.color_palette('Paired', 12)\n",
    "    cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer.probabilities_)]\n",
    "def KNN_outlier(train_X): \n",
    "#KNN using pypod\n",
    "    clf_name = 'KNN'\n",
    "    clf = KNN()\n",
    "    clf.fit(train_X)\n",
    "    y_train_scores = clf.decision_scores_\n",
    "    y_train_pred = clf.labels_\n",
    "    print('y pred',pd.Series(y_train_pred).value_counts())\n",
    "    \n",
    "# def KFold_outlier(X,Y):\n",
    "#     kf = KFold(len(Y), n_folds=5)\n",
    "#     for train_index, test_index in kf:\n",
    "#        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "#        LOF(X_train,y_train)\n",
    "       \n",
    "    \n",
    "def Outlier_detection():\n",
    "     #loading the imputed data set from CSV file\n",
    "    data=load_data()\n",
    "    train,test=split_train_test(data)\n",
    "     #splitting the dataset into train and test\n",
    "  \n",
    "    train_X = train[train.columns.difference(['y'])]\n",
    "    train_Y = train[['y']]\n",
    "    #KNN detection\n",
    "    KNN_outlier(X_train)\n",
    "    #LOF detection\n",
    "    LOF(train_X)\n",
    "    #HDB scan outlier detection\n",
    "    HDBSCAN(train_X)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "Outlier_detection()    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
